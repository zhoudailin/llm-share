{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0822815-9a9a-4c02-9fa5-056e4be380a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LLM基础分享\n",
    "## 一些基础的理论知识\n",
    "  \n",
    "  LLM(Large Language  Model)，即大型语言模型，本质上是一个深度神经网络，因此我们先来聊聊神经网络的一些基础知识，以便你对后面的一些具体细节有更为理性一些的认知。在计算机科学中，我们所谓的神经网络，指代的一般就是人工神经网络(ANN:Artificial Neural Network)。人工神经网络来自于对生物神经网络的模仿，是一种数学模型或者计算模型，用于对函数进行估计和近似。正如同生物神经网络一样，人工神经网络也是由一个个人工神经元组成的，每个人工神经元，有如下的简单结构:\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/Ncell.png?imageMogr2/thumbnail/400x\"/>\n",
    "</center>\n",
    "\n",
    "1. 其中$a_1$到$a_n$为输入的各个分量\n",
    "2. $w_1$到$w_n$是各个分量的权重\n",
    "3. b为偏置值\n",
    "4. f称为激活函数，一般是非线性函数，常用激活函数有sigmoid，tanh,Relu等\n",
    "\n",
    "如果将a和w作为一个整体(向量)，其实神经元就代表了一个很简单的函数\n",
    "$$t=f(wa+b)$$\n",
    "那么将很多神经元连接起来，就可以得到各式各样的神经网络\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/ANN.png?imageMogr2/thumbnail/400x\"/>\n",
    "</center>\n",
    "\n",
    "那么这些各种连接的神经元为啥能够产生所谓的智能呢，这时候就需要提到如下的一个定理\n",
    "> 通用近似定理：固定一元函数和一组仿射泛函的有限线性组合可以一致逼近单位超立方体中的任意连续函数\n",
    "\n",
    "其中的一元函数说的就是上面公式的$wa+b$部分，而仿射泛函就是指的激活函数，有限线性组合指的就是有限个神经元连接在一起。通用近似定理其实翻译成人话就是理论上证明了**不管一个连续函数多么复杂，都可以通过神经网络模拟出来**。而所谓智能其实就可以看成输入和输出的一个隐含关系。比如我们在使用LLM的时候，LLM本质就是一个将你的文字输入映射到另外一段文字输入的函数。而比如人脸识别，就是一个图像到一个身份信息，比如id的一个映射关系。\n",
    "\n",
    "通用近似定理告诉了我们神经网络可以模拟任何函数，却没有告诉我们如何得到这个函数。其实从上面描述我们可以看出来，如果我们确定了神经网络的连接方式，激活函数，权重$w$和偏置$b$等，那么就确定了一个神经网络了。一般来说，连接方式和激活函数是一个神经网络架构上的选择，不同名字的神经网络，比如GAN，RNN这些一般都有不同的连接方式和激活函数选择。而一旦确定了神经网络的架构，我们剩余的就是要**确定权重$w$和偏置$b$，而确定他们的过程，就是所谓的训练**。\n",
    "\n",
    "我们在后面选择模型的时候，会看到同一个模型会有不同的标识，比如`7B,20B,120B`等等，这个标识其实说的就是参数规模，这个参数主要指的就是权重和偏置。其中B指的是Bilion，所以比如一个神经网络写了7B，就表示它的参数规模是70亿左右。\n",
    "\n",
    "有了网络结构，有了参数规模，那么就是如何得到权重和偏置了。这里我简单解释下训练的过程是如何进行的，这个对于后面理解什么是微调是有帮助的。一般来说我们把机器学习可以通过是否给定答案来分为`有监督学习`和`无监督学习`。所谓有监督学习，就是你要告诉神经网络，你的输入正确的输出是什么，而无监督学习就只需要告诉输入。一般来说大部分任务都是需要监督学习完成的，因此我们平时会看到各种打标记，这个过程其实就是给神经网络准备正确答案的过程。\n",
    "\n",
    "当我们准备好了训练的数据，也就是一堆你所希望神经网络展现的正确的输入和输出之后，会将一定比例的数据留下用于验证最后结果，其他数据就会依次给神经网络用于训练。一般来说训练就是如下的一个过程\n",
    "1. 虽然初始化上面的各种参权重，偏置等参数\n",
    "2. 将某条数据的输入给到神经网络，得到这时候的输出\n",
    "3. 这时候的输出肯定和正确的输出是不一样的(否则就不需要训练了),那么可以通过某个指标计算出此时神经网络给出的结果和正确结果的误差有多大。\n",
    "4. 根据误差情况调整各种参数的值\n",
    "5. 重复上面2到4的步骤，直到数据都训练完成，或者输出的误差小于预定的一个目标\n",
    "6. 将剩余的验证用的数据输入此时的神经网络评估结果\n",
    "\n",
    "其实可以看出来，所谓的训练，就是通过调整参数的值，去尽量让神经网络给出的结果误差和正确误差越来越小的过程\n",
    "\n",
    "## LLM目前情况综述\n",
    "在知道了一些必要的基础知识之后，我们回到大语言模型本身。大语言模型(非多模态)属于人工智能自然语言处理(NLP)这个领域，其实这个领域历史很悠久，它的很多成果我们也一直在不知不觉中使用着。比如我们用过ElasticSearch，其中有个部分就是需要分词器，而分词就是NLP一个很重要的话题。\n",
    "\n",
    "其实在谷歌2017年发表《Attention Is All You Need》这篇论文之前，自然语言处理是没有计算机视觉火热的。当时GAN的发明，应该有人有印象当时有很多做风格迁移的，就是你给两张图片，比如一张是梵高的星空，一张是普通的照片，就会有个程序帮你把那种普通的照片变成梵高星空的风格。后来谷歌提出注意力机制之后不久又提出了Bert，这个当时也火了一段时间之后就没啥下文了，直到2023年ChatGPT的出现，让自然语言处理火出了圈。\n",
    "\n",
    "如今仅仅过去了一年左右，世面上优秀的大语言模型早已不是ChatGPT一家独大，而是呈现了一种百花齐放的姿态，其中最出名的有如下这些模型\n",
    "\n",
    "\n",
    "| 模型名 | 是否开源|是否多模态 | 开发商|许可证 |特点|\n",
    "|:--------:| :---------:|:---:|:--------:|:----:|:----:|\n",
    "| ChatGPT系列 | 否 |GPT4及其后面模型支持|OpenAI|/|遥遥领先|\n",
    "|Claude|否|claude3|Anthropic|/|前OpenAI高管创立，支持多模态，上下文窗口极大|\n",
    "|Llama系列|是|是|Meta|Llama 3 license|开源之王，从Llama衍生出了大量的二创作品，但是中文训练数据不太多，经常用英文回答|\n",
    "|Gemini系列|否|是|Google DeepMind|/|和前面的还是有差距，谷歌还是做基础研究写论文厉害|\n",
    "|Gemma|是|是|Google DeepMind|Apache 2.0|同上|\n",
    "|Mixtral 8x7B|是|是|MistralAI|Apache 2.0|和ChatGPT据说使用了同样的架构但是开源，最近很火|\n",
    "|Phi-2|是|否|微软|MIT|小模型，参数量小，有望嵌入边缘设备。中文支持不好。|\n",
    "|文心一言|否|是|百度|/|大量中文语料训练，实际表现一般|\n",
    "|百川|是|否|百川智能| Apache 2.0 |对话,创作,生成能力非常不错，代码和逻辑能力不好，算是国内模型比较优秀的|\n",
    "|Qwen|是|是|阿里云|Apache 2.0|系列模型，昨年第一个版本效果很差，最近2.5的质量不错|\n",
    "|ChatGLM|是|是|清华大学|Apache 2.0|对中文支持各类评测中最优秀，模型规模小，很容易跑起来|\n",
    "\n",
    "以上这些模型是从昨年到今年个人使用过的，其实还有很多著名的模型，比如腾讯的，商汤的，360的等，但是个人没深入使用过。上面的特点也只是个人感受，而且大模型这个东西，尤其是闭源的以服务形式提供的大模型，可能每天实际表现都会有偏差，所以以上也只能代表他们目前在个人心目中的情况。其实作为一个开发者而不是模型的研究者，个人认为不要将模型本身看得太过重要。当然这并不是说随便选个模型即可，毕竟LLM作为目前AI应用的心脏，重要性不言而喻。但是问题在于现在大模型层出不穷，将来可能会越来越多，如果作为开发者将所有时间都投入到了模型本身的研究上，有点本末倒置。何况目前因为langchain这类AI开发框架的成熟，在一个应用中将模型的选择和具体应用的实施解耦，使得替换一个模型变成一个很简单的事情。\n",
    "\n",
    "## 模型的部署\n",
    "一般来说，LLM都是python开发的应用程序，所以生产部署的话，实际就是走python部署的流程即可。注意这里的部署和后面的本地部署是有区别的，这里部署是以生产为目的，最终目标是跑起来暴露一套api给多人提供服务。而后面的本地部署，基本上意味着这个api只提供给单人服务。所以在使用的方案，工具，硬件需求等有很大的差别。\n",
    "\n",
    "不同的模型其实在他的仓库里面基本都说明了如何去部署，这里主要提两个点：\n",
    "\n",
    "第一个就是，前面说到了，预训练模型训练完成后会得到有参数，这个参数量是以十亿为单位的，很大。所以就有一个问题，这个参数存放在哪的，如果你去github看，会发现仓库里面不会有参数提供的，因为参数动辄几十G，大一点的上百G，甚至T级别。这里我们就要说到一个平台了，叫做huggingface。简单解释这是一个类似github的地方，被称为深度学习的github。但是和github不一样，这上面存放的不是代码，而是模型的参数，更准确地说，是transformer模型的参数。huggingface呢，写了个一个叫transformer的库，只要模型是transformer，就可以通过这个库加上参数运行。其实我们之前提过ChatGPT的T就是指代transformer，是谷歌提出来的基于注意力机制的一种模型架构。因为目前大部分各种各样的LLM基本都是transformer架构，所以他们都可以通过huggingface提供的库启动，也因此他们的参数都存放在了huggingface。我们以通义千问7B版本为例，首先是他的[github仓库](https://github.com/QwenLM/Qwen/tree/main)：\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/Qwen_github.png?imageMogr2/thumbnail/800x\"/>\n",
    "</center>\n",
    "\n",
    "可以看出来这上面只有模型本身的一些代码，包括启动的，训练的，微调的，量化的等等。我们再去[huggingface看看](https://huggingface.co/Qwen/CodeQwen1.5-7B/tree/main)：\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/Qwen_huggingface.png?imageMogr2/thumbnail/800x\"/>\n",
    "</center>\n",
    "\n",
    "会看到这上面有几个很大的文件，这些就是模型的权重信息，剩下的是一些描述模型结构等其他信息的文件。这样的话只需要有huggingface的transformer库，加上模型的名字就可以运行模型本身了，因为权重信息已经存放在huggingface了。其实即使你不直接通过huggingface去调用模型，你去看Qwen里面的代码，它封装好的api也是通过huggingface去调用的。通过如下代码就可以加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990b610-1ff0-44ad-ac44-45c571983d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-7B\")\n",
    "prompt = \"简单介绍下你自己.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6fd77-00db-4064-9038-a0e0ff60ff02",
   "metadata": {},
   "source": [
    "如果你的显卡是4090这种有24G左右显存，你可以尝试运行上面代码，否则会出现显存不足。即使你显存充足，依然可能会遇到一个问题，就是huggingface的访问问题，在国内huggingface是不能直接访问的，这里一般有两种处理方式，第一种依然使用huggingface，但是通过镜像源，或者手动下载权重文件到本地，又或者挂代理等方式处理。另外一种直接放弃huggingface，使用其他平台，比如阿里的modelscope魔搭社区，就可以看成阿里做的国内版huggingface，虽然没有huggingface那么多能力，但是不会存在访问问题。这里我们通过镜像站的方式来解决这个问题，但是因为我的显卡是不够运行Qwen-7B这个模型的非量化版本的，我们去云平台租用一台GPU服务器来完成演示，顺便分享些云平台使用的小tips。\n",
    "\n",
    "<center>\n",
    "    <video src=\"https://llmshare.zhoudailin.cn/medias/%E9%83%A8%E7%BD%B2.mp4\" width=\"800\" controls></video>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e7f84-751c-475a-b7a4-07662951389b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 如何微调模型\n",
    "前面我们说了训练，大家也应该知道训练的本质究竟是啥。一般来说市面上的LLM模型的训练素材都不是特化了，除了辅助代码编程类的LLM可能会使用大量的代码进行训练。因此这些LLM一般来说是通用的，可以用于任何问题，但是这也意味着他们不具有某些领域知识，也不知道训练完成后新的知识。比如某个太具体的细分专业的知识，我是学电气工程的嘛，所以很多同学在国家电网，当时GPT4才出来的时候我就开了会员，然后让同学给了些电网的题目给GPT4回答，结果当然是一塌糊涂。又比如你问哪怕最好的大语言模型如何使用大禹，它可能会给你一本正经胡说八道，因为它根本不知道大禹是公司的一个内部基于k8s的系统。又比如说前不久GPT4o刚出来嘛，如果你这时候去问很多模型关于GPT4o的信息，他们应该也是不知道的。那么这时候我们就可以对模型进行微调。\n",
    "想象一下，你现在手上有个语言模型，你很满意，但是你希望它能有一些前端的领域知识，但是它好像就是不懂前端。可能这个模型训练的时候假设用了10亿条数据，其中可能和前端相关的数据聊聊无几。它训练完成了的到了目前的参数。如果你这时候再去准备比如1万条前端的数据，在目前的参数的基础上继续进行训练，那么就约等于这1万条数据本来就在这10亿训练数据中一样，而你也不需要花费10亿条数据的训练时间，这部分时间前面的训练已经付出了。那么这个过程就是微调，**本质就是你准备一些新的数据，然后继续训练，在之前参数的基础上更新出一个新的参数的过程**，从上面微调的过程可以看出有以下隐含信息：\n",
    "1. 每个模型训练的时候准备的数据格式是不一样的，因此你准备的数据格式要和你的基础模型训练的格式一致，这个基本每个模型下面都是`fine-tune`的指南，里面都会提到数据格式的问题\n",
    "2. 你的数据量不能太多也不能太少。这个很好理解，假设你说我很有钱，无所谓之前10亿条数据花费多少时间，我再准备100亿条领域知识喂给模型，这样出来的模型结果可能就会将之前已经学到的东西给遗忘掉(产生所谓的过拟合)。当然你的数据量也不能太小，比如你就期望准备10条数据，让LLM学会领域知识，这个确实想想也不太靠谱。\n",
    "3. 微调是一个很耗费时间和钱的过程，对于开发工程师来说除非你准备在这上面深耕，否则让更专业的人或者手上有更多资源的人来做可能会更好。因为微调本来其实是一个需要很多知识才能做到的事情，模型一般是python写成，然后使用了pytorch这样的框架，所以你至少得懂得python和pytorch。然后微调的过程中还有很多参数可以控制，这些参数如何设置，需要你多少对网络结构有一些感性认知。最后就是，其实从最开始的通用近似定理我们可以看出来，我们所谓的智能是一种不可以解释的涌现智能，而目前也没有任何定量的理论指导如何会让一个模型的表现变得更好，很多东西都是定性的，形而上的，带一些玄学色彩的，大家应该听过那些搞模型训练的人，把训练的过程称为\"炼丹\"，其实也是这个原因。虽然目前有很多开源的项目降低了微调需要的学习成本，但是微调还有另外一个巨大的硬件成本，这个是开源解决不了的，除非你有资源，有好的显卡。否则尝试一次的成本非常高，而且得到的结果也可能只能用于学习而已。\n",
    "\n",
    "虽然上面不建议以开发应用为目的的工程师太过死磕微调，但是微调作为一个改变LLM行为的重要手段，也是需要掌握的。上面也提到过，训练和微调都是非常吃硬件资源的过程，所以很多研究者提出了一些微调的方法，可以降低微调过程中的硬件要求。同时也有很多开源项目的出现，直接降低了微调所需要的知识。\n",
    "\n",
    "#### LoRA\n",
    "首先第一种方法就是跟着之前训练的过程继续进行，调整所有参数，这个被称为`Full fine-tune`。因为需要调整所有参数，这些参数都需要占用显存，因此需要的硬件资源极高，但是也会带来相对最好的结果表现。这个对于企业来说如果有大量的显卡资源，肯定是最优选择，但是对于个人学习来说，我们更推荐其他微调方法，比如LoRA。接下来的示例我们也会以LoRA作为例子去微调一个Llama3。\n",
    "\n",
    "所谓LoRA(Low Rank Adapter)其实是基于研究者发现，对于一个大型的神经网络模型，调整它部分参数可以达到和调整全部参数一样的效果，也就是有一个极小的内在维度。LoRA作者受此启发，认为训练参数有个很小的内在秩，因此可以通过矩阵分解来用少量的数据替代大量的数据。\n",
    "\n",
    "为了通俗地解释LoRA的原理，我们回到最初的网络结构图。我们图示的示例网络是有三层，其中第一层是有3个神经元，第二层是有4个。它们之间一共有12，也就是$3\\times 4$条连线。这些连线本质上代表的是啥呢回顾一开始的神经元，其实这些连线就是代表了权重$w$，如果你没有想通这点，也无所谓，反正知道每条线上应该有个隐藏的数字，这个数字决定了当前输出的多少比例进入到下个神经元。接下来就需要回顾一下你大学的线性代数知识了，这$3\\times 4$个数字，可以表示为一个$3\\times 4$的矩阵。其实神经网络的参数很多都是以矩阵形式作为整体计算的，因为GPU很擅长处理这种矩阵的并行运算。图上的结构有三层，我们可以从中看出两个矩阵，而且两个矩阵大小都不是很大。但是实际的大型模型，里面可能存在数十个甚至上百个这样的矩阵，且每个矩阵都非常大。\n",
    "\n",
    "为了解释LoRA，你需要再回忆一件事，就是矩阵的乘法。我们知道两个矩阵$A$和$B$，如果$A$是$m*n$的矩阵，而$B$是$n*s$的矩阵，那么$AB$会得到$m*s$的矩阵：\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "有了这些准备，那么LoRA就很好理解了，其实它就是通过反过来使用矩阵乘法，将一个大矩阵分解成了两个小矩阵的乘积。假设我们需要训练的模型里面有个$10000*10000$的巨大矩阵，我们可以将它分解为一个$10000*1$和一个$1*10000$的矩阵，这样我们需要调整的参数就从$10000*10000$变成了$10000*2$。这个1其实就是所谓的秩，这也就是LoRA名字低秩适配器的由来。当然[实际情况](https://zhuanlan.zhihu.com/p/646791309)下，我们的秩可能取值不一定取1那么极端，但是根据实际结果表明，即使对很小的秩，比如2到9这种个位数，LoRA也有很好的效果。\n",
    "\n",
    "明白了理论，我们就来实际操作下如何微调:\n",
    "\n",
    "<center>\n",
    "    <video src=\"https://llmshare.zhoudailin.cn/medias/%E5%BE%AE%E8%B0%83.mp4\" width=\"800\" controls></viedo>\n",
    "</center>\n",
    "        \n",
    "## 如何量化模型\n",
    "完成了微调之后，我们还有个问题没有处理，那就是如何低成本得让模型运行起来。这时候我们就需要对模型进行量化。所谓量化，简单来说就是压缩数据。从上面我们可以看到，最消耗资源的部分就是模型中巨量的参数，这些参数一般来说是16位浮点，或者32位浮点或者混合浮点(amp)，他们每个数字所占用的空间都比较大，如果我们能用比较小的空间来表示这些数字，就可以大大节约资源。当然，这种用短位数去表达长位数，肯定会带来舍入误差的，那么问题在于这个误差对模型效果有没有影响，影响有多大。第一个当然是有影响的，所以量化过后的模型一般都是用于本地学习或者本地日常使用，生产等要求比较高但是资源相对充分的环境还是尽量使用未量化的版本。第二点，影响有多大。因为量化其实是有个量化参数的，就像压缩率。比如我模型本来是16位浮点数。你用8位int去代替，这时候效果可能还可以，如果你换成了4位int，可能效果就会更差些。一般来说根据统计数据量化4Q(用4位int)的版本比非量化的效果会平均差10%-20%。这个结果对于本来来说肯定是可以容忍的。量化一般通过llama.cpp进行，我们来进行操作\n",
    "\n",
    "<center>\n",
    "    <video src=\"https://llmshare.zhoudailin.cn/medias/%E9%87%8F%E5%8C%96.mp4\" width=\"800\" controls></viedo>\n",
    "</center>\n",
    "\n",
    "## 如何本地部署和使用LLM\n",
    "如果你只是一个使用者，或者只是想用api去完成你的某个作品，那么上面这些微调，量化可能对你来说不是太有吸引力，你需要的其实是一个可以调用的api。最初大家基本都使用ChatGPT的api，因为那时候ollama这种东西还没出现。虽然ChatGPT的账户每个都送了5美刀，但是相信当时去折腾的人还是很快就会折腾完这5美刀。如果只是为了学习，那么肆无忌惮地调用OpenAI的api的话，还是需要付出很多钱的。现在有了ollama，你可以不需要有昂贵的硬件，不需要懂python，也不需要花一分钱去买api服务，就可以本地使用大模型去进行学习，研究和使用。\n",
    "\n",
    "ollama的安装，按着[官网](https://ollama.com/download)的实例一步步进行即可。ollama的很多操作非常像docker，如果你平时docker用得比较多，应该会比较亲切。ollama其实使用了llama.cpp作为引擎，就是我们上面提到过的那个llama.cpp提供的main。然后它又模仿了dockerfile，做了一个modelfile的东西，让你通过modelfile可以描述如何将一个语言模型修改为另外一个模型。\n",
    "\n",
    "ollma的一些常用操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03624b2-a730-4e7b-ba92-fda6703f1ce3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 \tID          \tSIZE  \tMODIFIED     \n",
      "qwen:14b             \t80362ced6553\t8.2 GB\t46 hours ago\t\n",
      "llama3:latest        \ta6990ed6be41\t4.7 GB\t5 days ago  \t\n",
      "gemma:7b             \t430ed3535049\t5.2 GB\t2 months ago\t\n",
      "codellama:13b        \t9f438cb9cd58\t7.4 GB\t3 months ago\t\n",
      "codellama:7b         \t8fdf8f752f6e\t3.8 GB\t3 months ago\t\n",
      "llama2-chinese:13b   \t990f930d55c5\t7.4 GB\t3 months ago\t\n",
      "llama2-chinese:latest\tcee11d703eee\t3.8 GB\t3 months ago\t\n",
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏    0 B/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏  38 KB/4.7 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏  38 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏  38 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏  63 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏  63 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 100 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 167 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 331 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 460 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 565 KB/4.7 GB   19 KB/s  67h42m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 874 KB/4.7 GB   19 KB/s  67h41m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 976 KB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 1.5 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 2.0 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 2.2 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 2.7 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 3.3 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 3.7 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 3.7 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 3.9 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 3.9 MB/4.7 GB  325 KB/s   3h58m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 4.2 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 4.4 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 4.6 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 5.0 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 5.2 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 5.3 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a...   0% ▕                ▏ 6.2 MB/4.7 GB  1.0 MB/s   1h14m\u001b[?25h^C\n"
     ]
    }
   ],
   "source": [
    "# 查看当前本地有哪些模型\n",
    "!ollama list\n",
    "\n",
    "# 拉取一个模型\n",
    "!ollama pull llama3\n",
    "\n",
    "# 删除一个模型\n",
    "!ollama rm llama2-chinese:13b\n",
    "\n",
    "# 启动一个模型\n",
    "!ollama run llama3 (这里可以直接跟上问题，也可以不跟，进入交互式对话)\n",
    "\n",
    "# 查看资源占用（最新版本）\n",
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4e877-7319-4b38-afd4-30eb1d20dfd5",
   "metadata": {},
   "source": [
    "也可以通过restful接口去访问ollama，因为ollama是golang写了，里面自带了一个gin写的服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ec4834-6b39-4d85-9e63-3e8f8ef562da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"llama3\",\"created_at\":\"2024-05-21T15:56:15.851318233Z\",\"response\":\"Nice to meet you! 😊 I'm LLaMA, a large language model trained by a team of researcher at Meta AI. My name is derived from \\\"Large Language Model Application,\\\" and I'm here to assist and chat with users like you.\\n\\nI was trained on a massive dataset of text from the internet, which allows me to understand and generate human-like language. My training data includes a wide range of topics, styles, and genres, making me a versatile conversational AI.\\n\\nHere are some fun facts about me:\\n\\n1. **I'm a big brain**: I have over 17 billion parameters, which is a lot! This allows me to understand complex sentences, recognize patterns, and generate responses that are both informative and engaging.\\n2. **I can converse in multiple styles**: Whether you want a formal conversation or a more casual chat, I can adapt my tone and language to match your preferences.\\n3. **I'm a knowledge hub**: I've been trained on a vast amount of text data, which means I have access to a wide range of information on various topics, including history, science, technology, art, and more.\\n4. **I'm constantly learning**: Through user interactions like this one, I learn and improve my language understanding and generation capabilities.\\n\\nSo, what's your story? What would you like to talk about or ask me? I'm all ears (or rather, all text)! 🤖\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,198,198,117814,17297,57668,102099,128009,128006,78191,128007,198,198,46078,311,3449,499,0,27623,232,358,6,76,445,8921,4940,11,264,3544,4221,1646,16572,555,264,2128,315,32185,520,16197,15592,13,3092,836,374,14592,505,220,1,35353,11688,5008,7473,1359,323,358,6,76,1618,311,7945,323,6369,449,3932,1093,499,13,198,198,40,574,16572,389,264,11191,10550,315,1495,505,279,7757,11,902,6276,757,311,3619,323,7068,3823,12,4908,4221,13,3092,4967,828,5764,264,7029,2134,315,13650,11,9404,11,323,36744,11,3339,757,264,33045,7669,1697,15592,13,198,198,8586,527,1063,2523,13363,922,757,25,198,198,16,13,220,334,40,6,76,264,2466,8271,96618,358,617,927,220,1114,7239,5137,11,902,374,264,2763,0,1115,6276,757,311,3619,6485,23719,11,15641,12912,11,323,7068,14847,430,527,2225,39319,323,23387,13,198,17,13,220,334,40,649,95340,304,5361,9404,96618,13440,499,1390,264,16287,10652,477,264,810,16736,6369,11,358,649,10737,856,16630,323,4221,311,2489,701,19882,13,198,18,13,220,334,40,6,76,264,6677,19240,96618,358,6,588,1027,16572,389,264,13057,3392,315,1495,828,11,902,3445,358,617,2680,311,264,7029,2134,315,2038,389,5370,13650,11,2737,3925,11,8198,11,5557,11,1989,11,323,810,13,198,19,13,220,334,40,6,76,15320,6975,96618,17331,1217,22639,1093,420,832,11,358,4048,323,7417,856,4221,8830,323,9659,17357,13,198,198,4516,11,1148,6,82,701,3446,30,3639,1053,499,1093,311,3137,922,477,2610,757,30,358,6,76,682,25212,220,7,269,4856,11,682,1495,42395,11410,97,244,128009],\"total_duration\":4399015385,\"load_duration\":593768,\"prompt_eval_duration\":16485000,\"eval_count\":290,\"eval_duration\":4341068000}{\"error\":\"invalid character 'ä' looking for beginning of value\"}"
     ]
    }
   ],
   "source": [
    "# 生成一个回答\n",
    "!curl http://localhost:11434/api/generate -d '{\"model\": \"llama3\",\"prompt\":\"介绍下你自己\",\"stream\":false}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d699155f-7d49-4769-9607-37c4d97516db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"llama3\",\"created_at\":\"2024-05-21T15:59:04.496175848Z\",\"message\":{\"role\":\"assistant\",\"content\":\"😊\\n\\nTo gracefully shutdown or restart Nginx, you can use the following methods:\\n\\n**Method 1: Using the `nginx -s` command**\\n\\nYou can use the `-s` flag followed by one of the following options to control how Nginx shuts down:\\n\\n* `quit`: Quit immediately\\n* `stop`: Stop accepting new connections and finish processing existing requests\\n* `restart`: Restart Nginx with a full configuration reload\\n\\nExample:\\n```bash\\nnginx -s quit\\n```\\nThis will shut down Nginx gracefully, allowing existing requests to complete.\\n\\n**Method 2: Using the `kill` command**\\n\\nYou can send a signal to the Nginx process using the `kill` command. This method is useful when you want to restart Nginx quickly without waiting for pending requests to finish.\\n\\nExample:\\n```bash\\nkill -SIGUSR1 \\u003cnginx_pid\\u003e\\n```\\nReplace `\\u003cnginx_pid\\u003e` with the actual process ID of your Nginx instance.\\n\\n**Method 3: Using a script**\\n\\nYou can create a simple script that sends the `HUP` signal to the Nginx process, which will cause it to reload its configuration and shut down cleanly.\\n```bash\\n#!/bin/bash\\n\\nnginx_pid=$(pgrep -f nginx)\\nkill -HUP $nginx_pid\\n```\\nSave this script as, for example, `shutdown_nginx`, make it executable (`chmod +x shutdown_nginx`), and then run it when you want to shut down Nginx.\\n\\nRemember to replace `\\u003cnginx_pid\\u003e` with the actual process ID of your Nginx instance if you choose Method 2 or create a script.\\n\\nThese methods allow you to control how Nginx shuts down, ensuring that existing requests are completed before the server exits.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":5639399667,\"load_duration\":598694,\"prompt_eval_count\":11,\"prompt_eval_duration\":63606000,\"eval_count\":365,\"eval_duration\":5418714000}"
     ]
    }
   ],
   "source": [
    "# 生成一个对话\n",
    "!curl http://localhost:11434/api/chat -d '{ \\\n",
    "  \"model\": \"llama3\", \\\n",
    "  \"messages\": [ \\\n",
    "    { \"role\": \"user\", \"content\": \"nginx如何优雅停机\" } \\\n",
    "  ], \\\n",
    "  \"stream\":false \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea759c0-f40a-4cec-9b5a-ddc0d4a07333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"embedding\":[-0.09809943288564682,0.2309582680463791,0.23924008011817932,-0.040105562657117844,-0.1428043395280838,-0.056375935673713684,0.6265559792518616,-0.06985802203416824,-0.07132928818464279,-0.3343028426170349,0.07455319166183472,-0.4658896327018738,0.4492717981338501,0.10958538204431534,-0.37046751379966736,-0.20859773457050323,-0.05134684965014458,0.24048760533332825,-0.16589277982711792,0.22192054986953735,-0.1815894991159439,-0.1399330049753189,0.03935636952519417,-0.018299296498298645,-0.013934653252363205,0.16874642670154572,-0.3250887989997864,0.061025336384773254,0.3044147491455078,-0.059163372963666916,-0.16490723192691803,0.4930843412876129,-0.04848567768931389,-0.3056707978248596,0.13449542224407196,-0.08086539804935455,0.256054550409317,-0.12067772448062897,0.12770183384418488,-0.009993761777877808,0.015529081225395203,-0.31010493636131287,0.28019145131111145,-0.07297089695930481,-0.17617563903331757,-0.1567864567041397,0.10576505213975906,-0.15817974507808685,0.279777854681015,0.14774248003959656,0.07460891455411911,-0.2181270718574524,-0.20327286422252655,0.537140965461731,0.30084341764450073,0.040019962936639786,-0.2905174195766449,-0.09581281244754791,0.06754401326179504,-0.1784067004919052,0.04759383574128151,0.05096801370382309,0.37536269426345825,-0.07652956992387772,-0.2468608319759369,0.11001358926296234,-0.059257447719573975,-0.27222830057144165,0.05407581105828285,0.1408076286315918,0.12946802377700806,0.2550104260444641,-0.25809532403945923,0.1255209892988205,0.010936185717582703,0.04233153164386749,0.22661566734313965,-0.2749979496002197,-0.18986113369464874,-0.20192448794841766,-0.003410354256629944,-0.0065308623015880585,0.20810618996620178,0.23767516016960144,-0.31476959586143494,-0.01150486059486866,-0.31590524315834045,-0.10356662422418594,-0.06611205637454987,-0.055304206907749176,0.18488627672195435,-0.014530874788761139,0.005824679508805275,-0.08646570146083832,0.13168171048164368,-0.29484647512435913,0.007446166127920151,-0.11827081441879272,-0.32404467463493347,0.5401648283004761,0.04212940111756325,-0.015585590153932571,0.06514929234981537,-0.15243537724018097,-0.28413403034210205,-0.26181331276893616,-0.10745327919721603,0.03431202843785286,0.15789330005645752,-0.0763804093003273,-0.17630785703659058,-0.4560789465904236,0.0004224805161356926,-0.12646108865737915,-0.19700728356838226,0.2103249430656433,-0.0013712607324123383,0.11750467121601105,-0.07847204804420471,0.09699741005897522,0.24812191724777222,0.06759688258171082,-0.19571907818317413,0.263892263174057,-0.0694173201918602,-0.27135029435157776,0.054199546575546265,1.933348783108863e-32,-0.3302687406539917,-0.010746344923973083,0.10619727522134781,-0.2656431794166565,-0.11924713850021362,-0.045561790466308594,-0.01756027340888977,0.029408372938632965,-0.14509767293930054,-0.1091516762971878,-0.10603200644254684,0.21542395651340485,-0.4917283356189728,0.1397363841533661,0.14423832297325134,-0.23045316338539124,-0.07086485624313354,-0.08107049018144608,-0.04671948030591011,0.09802310168743134,0.24028420448303223,0.1665140986442566,-0.07226832211017609,0.2832595109939575,-0.0942339152097702,-0.2151319831609726,0.07694409787654877,0.06674937903881073,-0.051026493310928345,0.09281180799007416,-0.07866019010543823,-0.12273546308279037,-0.2609090507030487,-0.16580727696418762,-0.33681824803352356,0.08163492381572723,0.011969503946602345,-0.16794130206108093,-0.017699263989925385,0.055998794734478,-0.23910871148109436,0.1100788414478302,-0.22477149963378906,-0.49556583166122437,0.2640143036842346,0.25639355182647705,-0.1160096749663353,-0.18308788537979126,-0.032079704105854034,-0.24839311838150024,-0.09241178631782532,0.1071176677942276,-0.05168076604604721,0.2292191982269287,0.14402814209461212,0.11243490129709244,0.2260054647922516,0.060050494968891144,-0.1736094057559967,-0.08870311081409454,0.24482682347297668,-0.2927480638027191,0.14560110867023468,-0.10036475956439972,-0.0466398149728775,0.1825064867734909,0.2370230108499527,-0.2857239544391632,-0.3410319983959198,-0.09268181771039963,-0.452444851398468,0.15237773954868317,0.10546842217445374,0.07466421276330948,-0.01022089272737503,-0.14522820711135864,-0.2906733751296997,0.18331840634346008,-0.06784211844205856,-0.2756081521511078,-0.06365090608596802,0.06757177412509918,-0.12443047016859055,-0.044795647263526917,0.013530394062399864,0.2728351056575775,0.05535215139389038,0.028788000345230103,-0.17080247402191162,0.15402698516845703,-0.8211427927017212,-0.14125511050224304,-0.14723509550094604,-0.07065324485301971,-0.24418272078037262,-2.8474405205519304e-32,-0.2784082889556885,0.018400996923446655,0.10063257068395615,0.020184144377708435,-0.11442272365093231,-0.08212309330701828,-0.0446091964840889,0.3069242537021637,0.34948334097862244,0.18414556980133057,0.10220298171043396,-0.2885580062866211,-0.20656034350395203,0.2825915813446045,-0.014231279492378235,0.26147177815437317,0.45465850830078125,0.36457058787345886,-0.2590208053588867,-0.16619902849197388,-0.16226641833782196,-0.24661675095558167,-0.3384590148925781,0.21439805626869202,-0.1796232908964157,0.12176868319511414,0.28790032863616943,0.24138595163822174,-0.04683621972799301,0.04394051432609558,-0.5860114097595215,-0.1725788414478302,-0.40715426206588745,0.16228844225406647,-0.16979902982711792,-0.14002114534378052,-0.03937501460313797,0.041402243077754974,-0.06273051351308823,-0.07291581481695175,0.1953631192445755,-0.01717211678624153,0.060857534408569336,0.28395435214042664,0.08652672171592712,-0.11339066922664642,-0.30295029282569885,-0.2400875836610794,0.06791554391384125,0.19511310756206512,-0.046253353357315063,0.048344992101192474,0.09353812038898468,-0.3112996816635132,0.02905581146478653,0.03403658792376518,-0.07276665419340134,0.01755264587700367,0.06763078272342682,-0.022240187972784042,0.13697075843811035,0.04514820873737335,-0.03700878471136093,0.4156699776649475,-0.06615273654460907,0.23457542061805725,-0.09187276661396027,-0.036544352769851685,-0.005837604403495789,-0.04089713096618652,0.3164306879043579,-0.06797444075345993,-0.38190287351608276,0.30503347516059875,-0.10333102196455002,0.13944484293460846,-0.026243794709444046,0.08215868473052979,-0.07258698344230652,-0.10822535306215286,-0.12939852476119995,-0.24740491807460785,-0.00899847224354744,0.17282292246818542,0.09341183304786682,-0.02448268234729767,0.4474005103111267,0.5128684639930725,0.09755485504865646,-0.2868494391441345,0.25520026683807373,0.3174544870853424,0.02925921231508255,-0.40823906660079956,0.04085306078195572,-1.0057767951821006e-7,-0.2163066267967224,-0.17582307755947113,0.12534238398075104,-0.10592438280582428,0.01913154497742653,-0.014398977160453796,-0.015657098963856697,-0.04767376929521561,0.16974478960037231,-0.265817791223526,0.4918774962425232,-0.009681334719061852,-0.09244568645954132,0.1526184380054474,-0.062027089297771454,0.19719117879867554,-0.19726575911045074,0.09591367095708847,0.23823663592338562,-0.17870120704174042,0.004721230361610651,0.01342318207025528,-0.11665801703929901,0.020760446786880493,-0.06945884227752686,0.11513844132423401,0.00664103776216507,0.42016512155532837,-0.0003750203177332878,0.13458184897899628,0.11655546724796295,-0.13172239065170288,-0.2956024408340454,-0.1656123548746109,-0.20399071276187897,-0.08147390186786652,-0.08365622162818909,0.044397108256816864,0.08247226476669312,-0.367145299911499,-0.08883446455001831,-0.003651045262813568,0.37718653678894043,-0.049124687910079956,-0.030713528394699097,-0.14566634595394135,0.4165378212928772,0.23330077528953552,0.16935408115386963,-0.3391098976135254,0.16408006846904755,0.19016623497009277,0.3573320508003235,-0.0911981612443924,0.05338933318853378,0.1992243528366089,0.016390718519687653,0.29626011848449707,0.11277729272842407,0.22075772285461426,0.027920790016651154,-0.028109997510910034,0.248047336935997,0.14339928328990936]}"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "!curl http://localhost:11434/api/embeddings -d '{ \\\n",
    "  \"model\": \"all-minilm\", \\\n",
    "  \"prompt\": \"nginx如何优雅停机\" \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aee4f5-3b02-4dd0-ad94-2d4fa8a8706e",
   "metadata": {},
   "source": [
    "除了以上三个最常用的api，还有一系列关于镜像拉取，查看，删除等的接口，这些接口在你编写一个需要控制用户镜像的应用的时候还是会有很大用处的，参考[完整的document](https://github.com/ollama/ollama?tab=readme-ov-file)\n",
    "\n",
    "当然其实ollama官方提供了python和node的包，可以直接使用，但是个人感觉ollama的这两个包有点鸡肋。主要以为langchain的存在，如果你用的是python和nodejs，这两个langchain都有官方支持，不如直接用langchain。如果你用的其他语言，那么ollama也不支持，那可能还是需要自己依靠restful接口去封装。所以ollama的node包和python包我就不介绍了，我们接下来说下ollama的另外一个特色，就是模仿dockerfile的modelfile\n",
    "\n",
    "相比于dockerfile非常繁多和复杂的指令，ollama的modelfile目前只有7个指令：\n",
    "##### From\n",
    "和dockerfile一样，From指定一个基础镜像，这个镜像除了可以是镜像，还可以是gguf文件。\n",
    "\n",
    "```dockerfile\n",
    "FROM llama3:8b\n",
    "```\n",
    "\n",
    "```dockerfile\n",
    "FROM ./qwen-frontend.gguf\n",
    "```\n",
    "\n",
    "##### PARAMETER\n",
    "提供了很多可以调整的[底层参数](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)一般来说比较少调整\n",
    "\n",
    "##### TEMPLATE\n",
    "\n",
    "TEMPLATE可以用来构造一个提示词模板，也比较少用，因为langchain有更完整的提示词模板\n",
    "\n",
    "##### SYSTEM\n",
    "上下文描述，如果你在连续对话的时候需要模型都去记住某个上下文，就可以使用它。比如从上面例子可以看出来llama3经常容你你问中文，它回答英文，这时候可以用SYSTEM指令，用llama3作为基底镜像构造个新的镜像，它英文回答的问题就会立刻缓解很多\n",
    "\n",
    "```dockerfile\n",
    "FROM llama3:8b\n",
    "SYSTEM \"\"\"你将扮演一个只会说中文的角色，无论我用英文还是中文问你，你都用中文回答我\"\"\"\n",
    "```\n",
    "\n",
    "##### ADAPTER\n",
    "这个可以将微调的LoRA适配器文件直接加载到FROM的基础文件上，但是必须是GGML格式，有兴趣可以自己去了解，个人平时没怎么用到过\n",
    "\n",
    "##### LICENSE\n",
    "就是LICENSE描述\n",
    "\n",
    "##### MESSAGE\n",
    "这个可以提供一段对话的历史，然后将它固化下来。比如我有一段很短的领域知识，LLM根本不可能知道，然后我可以通过对话告诉他，然后再问问题，如果这样的操作非常多，我们可以把提供问题这部分给固化下来。\n",
    "\n",
    "```dockerfile\n",
    "FROM llama3\n",
    "SYSTEM \"\"\"你将扮演一个只会说中文的角色，无论我用英文还是中文问你，你都用中文回答我\"\"\"\n",
    "MESSAGE user 我将提供一段文字给你，这段文字包含了我同事的一些信息，你根据我提供给你的文字回答问题。\n",
    "MESSAGE assistant 好的\n",
    "MESSAGE user 我的室经理是张文涛，我的组长是孙翊浩\n",
    "```\n",
    "将上面这段做成镜像后，每次启动都会自动进行前面的对话。然后你就可以直接对提供给他的知识进行问询，而不用每次都去提供一次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae6fa5-4355-4e4f-972c-9c2cffce5fa4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## langchain和RAG\n",
    "最后来说说langchain和RAG吧。自从昨年ChatGPT出来后不久，就有了langchain。那时候大家对于如何开发一个AI应用还是比较迷茫，langchain抽象出来了AI应用开发的六大组件：Models,Prompts,Indexs,Memory,Chains,Agents。其实严格来说langchain是社区各种想法的一个合集，不使用langchain你也可以进行开发，但是langchain提供了很多便利给你。而且随着社区对于AI应用开发的探索已经越来越多新的架构被提出，langchain也一直在改变，所以这六大组件只是目前langchain的六大组件，将来可能会变成七大，八大，谁知道呢。自从langchain出来之后就一直受到大家的关注和喜爱，github的star数也是一路飙升：\n",
    "<center>\n",
    "    <img src=\"https://llmshare.zhoudailin.cn/pictures/star-history-langchain.png?imageMogr2/thumbnail/800x\"/>\n",
    "</center>\n",
    "\n",
    "对于langchain开发的具体实施过程，这里就不再赘述，有兴趣可以去自己学习。我们来简单说下什么是RAG（Retrieval Augmented Generation）。同样的我们举个例子：\n",
    "\n",
    "假设你去问任何一个语言模型，一些你们公司内部的问题，他们都无法回答，因为他们没有学习过相关数据。但是假设，你提供相关数据给他们，就像上面ollama MESSAGE那样，他们就能根据你给的资料去回答问题。那么想想一下，你要做一个知识库，比如我存了一堆PDF电子书，他们可能是某个领域的，比如前端的。我可能把这些书看完了，也可能一个字都没看。我怎么能够让一个语言模型根据这些电子书的内容来回答我的前端问题呢。这里和上面的不一样，因为你不可能把所有的书全部扔个一个大模型，它们往往没有那么大的上下文窗口(现在也有窗口极大的模型，可以装下无限的资料，但是性能消耗依然是个问题)，那么就需要它自己去这些电子书里面找到和你问题最相关的那些片段，然后根据这些片段回答问题。比如我问它前端的Symbol怎么用，它可能就会去我提前存放的电子书里面找和symbol相关的内容，然后根据找到的内容来回答我的问题。这就是所谓检索增强生成(RAG)的含义。\n",
    "\n",
    "那么如何让LLM自己找到和问题相关的部分呢，这其实就是RAG的核心，也是影响RAG效果的最主要的一步。其实要做到这一点，我们常规也有手段可以做到，比如elasticsearch的倒排索引可以根据问题的关键词去检索片段，但是更多的我们使用embbeding的技术，embedding之前分享已经讲过，这里就只是再简单说下：简单来说就是可以通过embedding将文本片段，视频或者音频啥的，都转化为一个向量，然后你询问问题之后，将问题也转化为一个向量。然后计算各个文字片段向量和问题向量的接近值，就可以知道哪些片段和问题相关。所以embedding算法的选择对于RAG至关重要，当然RAG还有很多优化的手法，比如ReRank，结构化检索库等，以后可以单独分享。\n",
    "\n",
    "对于RAG还有一个问题，就是我们的向量存在哪里。如果你的被检索数据不多，计算出来的向量当然可以存在内存中。但是往往被检索数据是很多的，这时候一般会使用向量数据库。顾名思义向量数据库就是专门为向量检索而生的，它可以快速进行向量相似度计算，内置了各种向量相似度，是RAG应用除了embedding之外的另外一个核心组件。\n",
    "\n",
    "最后langchain官方提供了node和python两个语言的，社区提供了各种语言的版本。如果可以，建议直接使用python版本的，因为在AI这一块，python是有最好的生态的。比如举个例子，如果做一个PDF的知识库，你需要将PDF数据读取出来。PDF数据呢可能因你PDF质量，会有各种不同的结果。比如你的PDF做了ocr识别，那么就能读出里面文字信息。比如里面图片很多文字很少，那么你就需要单独思考这些图片怎么处理。同样是官方提供的langchain，在PDF的loader这一块，nodejs就非常简单，只有那么一两个很基本的pdf检索库，出来的效果呢就完全和你PDF本身质量挂钩。但是python版本的pdfloader因为有python生态支持，里面很多深度学习相关的方法，可以帮你做OCR识别，可以是别图片，代码等等等等。同样是PDFLoader，用python版本出来的数据质量会明显更好。同样的，前段时间看java的时候，在spring官网也看到了spring也做了一个叫spring AI的框架，组件基本上都和langchain一样分为那么几类，依托于java的生态，可能在部署或者其他方面会有一定优势，但是在深度学习这一块，个人觉得短时间内其他语言是动摇不了python的地位的，所以如果可以优先使用python版本的。\n",
    "## 结尾\n",
    "其实个人感觉折腾了快一年，如果你要做AI相关的开发，不管是学习，还是想给公司落地，或者参与开源生态提升自我，都要想清楚自己是要侧重啥。和四五年前不同，那时候做深度学习，你可能需要懂很多数学，python，机器学习的知识才能摸到门槛。但是现在，即使你完全不会这些，通过社区繁荣的生态，你也可以做点有意思的东西出来，比如我最早看到的bilibili的视频总结，就是一个前端大佬做出来的，用的纯js版本langchain，配合ffmpeg和其他开源社区的项目。还有各个公司，比如之前字节跳动分享的让LLM加入代码检视，这些都挺有意思的，也没有往日那么高的门槛，虽然数学，python，这些知识你知道了对你做大模型应用开发肯定是有益处的，所以如果你有想法，只是害怕或者觉得我办不到，那你大可以去放心尝试。但是作为结束，还是要泼一瓢冷水，也是最近社区看到的一句话：“不要高估一个技术在两年内能带给你的，也不要低估一个技术在十年内能改变的”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
